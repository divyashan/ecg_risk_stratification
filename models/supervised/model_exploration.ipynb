{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "../../../ecg_AAAI/models/supervised/eval.py:5: UserWarning: matplotlib.pyplot as already been imported, this call will have no effect.\n",
      "  matplotlib.use('Agg')\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import pdb\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.io import loadmat\n",
    "\n",
    "sys.path.insert(0, '../../../')\n",
    "from ecg_AAAI.parse_dataset.readECG import loadECG\n",
    "from ecg_AAAI.models.ecg_utils import get_all_adjacent_beats\n",
    "from ecg_AAAI.models.supervised.ecg_fi_model_keras import build_fi_model \n",
    "from ecg_AAAI.models.supervised.ecg_fc import build_fc_model\n",
    "\n",
    "from ecg_AAAI.models.gpu_utils import restrict_GPU_keras\n",
    "from ecg_AAAI.models.supervised.eval import evaluate_AUC, evaluate_HR, risk_scores\n",
    "from ecg_AAAI.models.supervised.ablation_helpers import *\n",
    "import tftables\n",
    "restrict_GPU_keras(\"3\")\n",
    "y_mode = \"mi\"\n",
    "model_name = \"fc\"\n",
    "splits = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "split_dir_prefix = \"/home/divyas/ecg_AAAI/datasets/splits/split_\"\n",
    "batch_size = 60\n",
    "day_thresh = 90\n",
    "input_dim = 256\n",
    "train_file = None\n",
    "test_file = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up directory structure in case it's not there\n",
    "fig_dir = \"/home/divyas/ecg_AAAI/models/supervised/figs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened file\n",
      "Loaded labels\n",
      "N blocks:  2\n",
      "N batches:  7\n"
     ]
    }
   ],
   "source": [
    "timer = 0\n",
    "n_results = 1\n",
    "result_dicts = []\n",
    "block_size = 1500\n",
    "split_num = \"0\"\n",
    "if train_file:\n",
    "    train_file.close()\n",
    "    test_file.close()\n",
    "split_dir = split_dir_prefix + split_num\n",
    "\n",
    "train_file = h5py.File(split_dir + \"/train.h5\", \"r\")\n",
    "test_file = h5py.File(split_dir + \"/test.h5\", \"r\")\n",
    "print(\"Opened file\")\n",
    "train_y = get_labels(train_file, y_mode, day_thresh)\n",
    "test_y = get_labels(test_file, y_mode, day_thresh)\n",
    "print(\"Loaded labels\")\n",
    "train_pos_idxs = np.where(train_y == 1)[0]\n",
    "x_train_pos = train_file['adjacent_beats'][list(train_pos_idxs)]\n",
    "x_train_pos = reshape_X(x_train_pos)\n",
    "y_train_pos = train_file[y_mode + '_labels'][list(train_pos_idxs)]\n",
    "y_train_pos = thresh_labels(y_train_pos, day_thresh)\n",
    "y_train_pos = np.array([[y_val]*3600 for y_val in y_train_pos]).flatten()\n",
    "\n",
    "n_train_pos = len(train_pos_idxs)\n",
    "batch_size = n_train_pos\n",
    "n_batches = int(block_size/batch_size)\n",
    "n_blocks = int(len(train_y)/block_size)\n",
    "print(\"N blocks: \", n_blocks)\n",
    "print(\"N batches: \", n_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           (None, 256, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 128, 2)            258       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 32, 2)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 16, 2)             258       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 4, 2)              0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "softmax (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 525\n",
      "Trainable params: 525\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from ecg_AAAI.models.supervised.ecg_cnn import build_cnn\n",
    "\n",
    "m= build_cnn((input_dim, 1))\n",
    "m.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading Block # 0\n",
      "Done getting batch\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'class_weight' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-79f0cda78073>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mx_train_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_train_neg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train_pos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0my_train_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_train_neg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_pos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Plotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'class_weight' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(n_results):\n",
    "    for j in range(n_blocks):\n",
    "        # Load 1000 patients into memory at a time\n",
    "        x_train_block, y_train_block = get_block(train_file, i, block_size, y_mode, day_thresh)\n",
    "        print(\"Finished loading Block #\", j)\n",
    "        for k in range(n_batches):\n",
    "            x_train_neg, y_train_neg = get_block_batch(x_train_block, y_train_block, batch_size, k) \n",
    "            print(\"Done getting batch\")\n",
    "            x_train_batch = np.concatenate([x_train_neg, x_train_pos])\n",
    "            y_train_batch = np.concatenate([y_train_neg, y_train_pos])\n",
    "            m.fit(x=x_train_batch, y=y_train_batch, epochs=1, verbose=True, batch_size=80000)\n",
    "    # Plotting\n",
    "    if i % 1 == 0:\n",
    "        print(\"Starting to plot: \")\n",
    "        py_pred = get_preds(m, test_file)\n",
    "        print(\"Finished test predictions\")\n",
    "        if len(patient_y) != len(py_pred):\n",
    "            fixed_length = min(len(patient_y), len(py_pred))\n",
    "            patient_y = patient_y[:fixed_length]\n",
    "            py_pred = py_pred[:fixed_length]\n",
    "\n",
    "        auc_score = roc_auc_score(patient_y, py_pred)\n",
    "        fig_path = get_fig_path(y_mode, day_thresh, split_num)\n",
    "\n",
    "        plt.hist(py_pred[np.where(all_y == 1)], color='red', alpha=.5, bins=20)\n",
    "        plt.title(\"[\" + y_mode +  \" positive] distribution of risk scores (90 days) AUC = \" + str(auc_score))\n",
    "        plt.xlim(0, 1)\n",
    "        plt.savefig(fig_path +\"/epoch_\" + str(i) + \"_positive\")\n",
    "        plt.clf()\n",
    "\n",
    "        plt.hist(py_pred[np.where(all_y != 1)], color='green', alpha=.5, bins=20)\n",
    "        plt.title(\"[\" + y_mode +  \" negative] distribution of risk scores (90 days) AUC = \" + str(auc_score))\n",
    "        plt.xlim(0, 1)\n",
    "        plt.savefig(fig_path +\"/epoch_\" + str(i) + \"_negative\")\n",
    "        plt.clf()\n",
    "        print(\"Finished plotting\")\n",
    "        result_dict = {'y_mode': y_mode, 'epoch': i, 'model': model_name, 'pauc': auc_score,\n",
    "                       'day_thresh': day_thresh, 'split_num': split_num}\n",
    "        result_dicts.append(result_dict)\n",
    "        pd.DataFrame(result_dicts).to_csv(\"results_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_pred = get_preds(m, test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_test_y = test_file[y_mode + \"_labels\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = np.percentile(py_pred, 75)\n",
    "dicts = []\n",
    "for d, pred in zip(orig_test_y, py_pred):\n",
    "    o = 1 if d > 0 else 0\n",
    "    r = 1 if pred > thresh else 0\n",
    "    dicts.append({'duration': d, 'observed': o, 'risk': r})\n",
    "data = pd.DataFrame(dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lifelines.NelsonAalenFitter: fitted with 1247 observations, 1149 censored>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = data[\"duration\"]\n",
    "E = data[\"observed\"]\n",
    "\n",
    "from lifelines import NelsonAalenFitter\n",
    "naf = NelsonAalenFitter()\n",
    "\n",
    "naf.fit(T,event_observed=E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: norm_delta = 0.00495, step_size = 0.95000, ll = -354.53909, seconds_since_start = 0.0\n",
      "Iteration 2: norm_delta = 0.00023, step_size = 0.95000, ll = -354.53778, seconds_since_start = 0.1\n",
      "Iteration 3: norm_delta = 0.00001, step_size = 0.95000, ll = -354.53778, seconds_since_start = 0.1\n",
      "Convergence completed after 3 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lifelines.CoxPHFitter: fitted with 1247 observations, 1149 censored>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lifelines import CoxPHFitter\n",
    "cph = CoxPHFitter()\n",
    "cph.fit(data, duration_col='duration', event_col='observed', show_progress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01200816962966578"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cph.hazards_['risk'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 256\n",
    "num_fc_0 = 2\n",
    "half_dims = (int(input_dim/2), num_fc_0)\n",
    "init_weights = np.concatenate([np.ones(half_dims), \n",
    "                               -1*np.ones(half_dims)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_bias = m.layers[2].get_weights()[1]\n",
    "model.layers[2].set_weights([init_weights, rand_bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_small = 320000\n",
    "small_x = np.concatenate([x_train_batch[:n_small], x_train_batch[-1*n_small:]], axis=0)\n",
    "small_y = np.concatenate([y_train_batch[:n_small], y_train_batch[-1*n_small:]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "129728/640000 [=====>........................] - ETA: 2:26 - loss: 0.5069 - acc: 0.7749"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-9ca899eaaca0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmall_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmall_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "m.fit(small_x, small_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1360800"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_batch.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
