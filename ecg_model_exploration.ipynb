{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import pdb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.io import loadmat\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "from ecg_AAAI.parse_dataset.readECG import loadECG\n",
    "from ecg_AAAI.models.ecg_utils import get_all_adjacent_beats\n",
    "from ecg_AAAI.models.supervised.ecg_fi_model_keras import build_fi_model \n",
    "from ecg_AAAI.models.supervised.ecg_fc import build_fc_model\n",
    "from ecg_AAAI.models.gpu_utils import restrict_GPU_keras\n",
    "from ecg_AAAI.models.supervised.eval import evaluate_AUC, evaluate_HR, risk_scores\n",
    "\n",
    "mode = 'two_beat'\n",
    "m_type = 'nn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Y\n",
    "hf = h5py.File('datasets/data.h5', 'r')\n",
    "y_train = np.array(hf.get('y_train'))\n",
    "y_test = np.array(hf.get('y_test')) \n",
    "hf.close()\n",
    "\n",
    "# Load X \n",
    "if mode == \"one_beat\":     \n",
    "    x_file = h5py.File('datasets/one_beat.h5', 'r')\n",
    "    all_test = h5py.File('datasets/all_test_one.h5', 'r')\n",
    "    n_beats = 1000\n",
    "    instance_length = 128\n",
    "elif mode == \"three_beat\":\n",
    "    x_file = h5py.File('datasets/three_beat.h5', 'r')\n",
    "    all_test = h5py.File('datasets/all_test_three.h5', 'r')\n",
    "    n_beats = 998\n",
    "    instance_length = 384\n",
    "elif mode == 'four_beat':\n",
    "    x_file = h5py.File('datasets/four_beat.h5', 'r')\n",
    "    all_test = h5py.File('datasets/all_test_four.h5', 'r')\n",
    "    n_beats = 997\n",
    "    instance_length = 512\n",
    "else: \n",
    "    x_file = h5py.File('datasets/two_beat.h5', 'r')\n",
    "    all_test = h5py.File('datasets/all_test_two.h5', 'r')\n",
    "    n_beats = 999 \n",
    "    instance_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: redo test train split\n",
    "X_train = np.array(x_file.get('X_train'))\n",
    "X_test = np.array(x_file.get('X_test')) \n",
    "x_file.close()\n",
    "\n",
    "all_test_patients = np.array(all_test.get('test_patients'))\n",
    "all_test_patient_labels = np.array(all_test.get('test_patient_labels'))\n",
    "all_test_pids = np.loadtxt('all_test_pids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4816"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_test_patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create balanced 80/20 split by moving some elements from test to train\n",
    "# TODO: make this more random? /even?\n",
    "train_add = all_test_patients[:4000].reshape(4000*n_beats, 1, instance_length) \n",
    "train_label_add = np.array([[g]*n_beats for g in all_test_patient_labels[:4000]])\n",
    "X_train = np.concatenate([X_train,train_add])\n",
    "y_train = np.concatenate([y_train, train_label_add.reshape(4000*n_beats)])\n",
    "\n",
    "all_test_patients = all_test_patients[4000:]\n",
    "all_test_patient_labels = all_test_patient_labels[4000:]\n",
    "all_test_pids = all_test_pids[4000:]\n",
    "\n",
    "# Load test patient metadata\n",
    "test_hf = h5py.File('datasets/test_pids.h5', 'r')\n",
    "test_pids = np.array(test_hf.get('pids'))\n",
    "test_patient_labels = np.array(test_hf.get('patient_labels'))\n",
    "test_hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading data...\n"
     ]
    }
   ],
   "source": [
    "# Pre-process data\n",
    "input_dim = X_train.shape[2]\n",
    "X_train = np.swapaxes(X_train, 1, 2)\n",
    "X_test = np.swapaxes(X_test, 1, 2)\n",
    "\n",
    "# Shuffle X_train\n",
    "new_order = np.arange(len(X_train))\n",
    "np.random.shuffle(new_order)\n",
    "X_train = X_train[new_order]\n",
    "y_train = y_train[new_order]\n",
    "\n",
    "# Shuffle X_test\n",
    "new_order = np.arange(len(X_test))\n",
    "np.random.shuffle(new_order)\n",
    "X_val = X_test[new_order][:3000]\n",
    "y_val = y_test[new_order][:3000]\n",
    "\n",
    "patient_outcomes = loadmat(\"./datasets/patient_outcomes.mat\")['outcomes'] \n",
    "survival_dict = {x[0]: x[4] for x in patient_outcomes}\n",
    "print(\"Done loading data...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Summary\n",
    "\n",
    "    ### Original dataset: n = 6354 \\\\  [6067, 287]\n",
    "    ### Paul dataset: n = 4786 \\\\ [5104, 132]\n",
    "\n",
    "    ### txt files: 4985\n",
    "    ### then in total the number of patients is 4975... weird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "627"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test %:  0.8689928959465106 0.13100710405348934\n",
      "Train/Test n: 4159000.0 627000.0\n"
     ]
    }
   ],
   "source": [
    "n_train = float(len(y_train))\n",
    "n_test = float(len(y_test))\n",
    "n = float(n_train + n_test)\n",
    "print(\"Train/Test %: \", n_train/n, n_test/n)\n",
    "print(\"Train/Test n:\", n_train, n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train \t p(y = 1):  0.04832892522240923\n",
      "Test \t p(y = 1): 0.32057416267942584\n"
     ]
    }
   ],
   "source": [
    "n_pos_train = len(np.where(y_train == 1)[0])\n",
    "pct_pos_train = n_pos_train/float(len(y_train))\n",
    "\n",
    "n_pos_test = len(np.where(y_test == 1)[0])\n",
    "pct_pos_test = n_pos_train/float(len(y_test))\n",
    "\n",
    "                \n",
    "print(\"Train \\t p(y = 1): \", pct_pos_train)\n",
    "print(\"Test \\t p(y = 1):\", pct_pos_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           (None, 128, 1)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "fc_1 (Dense)                 (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "softmax (Dense)              (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 261\n",
      "Trainable params: 261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if m_type == 'LR':\n",
    "    n_iter = 1\n",
    "    m = LogisticRegression()\n",
    "\n",
    "    X_train = np.squeeze(X_train, 2)\n",
    "    test_patients = np.resize(X_test, (627, 1000, input_dim))\n",
    "    train_m = lambda : m.fit(X_train, y_train)\n",
    "    score_m = lambda test_patients: risk_scores(m, test_patients)\n",
    "\n",
    "else:\n",
    "    n_iter = 5\n",
    "    m, embedding_m = build_fc_model((input_dim, 1))\n",
    "    m.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    m.summary()\n",
    "\n",
    "    test_patients = np.resize(X_test, (627, 1000, input_dim, 1))\n",
    "    class_weight = {0: 1.,\n",
    "                    1: 1/pct_pos_train}\n",
    "    train_m = lambda : m.fit(x=X_train, y=y_train, validation_data=(X_val, y_val), epochs=10, verbose=True, batch_size=10000, class_weight=class_weight)\n",
    "    score_m = lambda test_patients: risk_scores(m, test_patients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4159000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "4159000/4159000 [==============================] - 14s 3us/step - loss: 1.2917 - acc: 0.5061 - val_loss: 0.7032 - val_acc: 0.6900\n",
      "Epoch 2/10\n",
      "4159000/4159000 [==============================] - 12s 3us/step - loss: 1.2512 - acc: 0.6717 - val_loss: 0.6930 - val_acc: 0.6867\n",
      "Epoch 3/10\n",
      "4159000/4159000 [==============================] - 13s 3us/step - loss: 1.2373 - acc: 0.6855 - val_loss: 0.6799 - val_acc: 0.7133\n",
      "Epoch 4/10\n",
      "4159000/4159000 [==============================] - 13s 3us/step - loss: 1.2307 - acc: 0.6923 - val_loss: 0.6779 - val_acc: 0.7087\n",
      "Epoch 5/10\n",
      "4159000/4159000 [==============================] - 13s 3us/step - loss: 1.2272 - acc: 0.6945 - val_loss: 0.6719 - val_acc: 0.7177\n",
      "Epoch 6/10\n",
      "4159000/4159000 [==============================] - 14s 3us/step - loss: 1.2250 - acc: 0.6955 - val_loss: 0.6758 - val_acc: 0.7080\n",
      "Epoch 7/10\n",
      "4159000/4159000 [==============================] - 14s 3us/step - loss: 1.2235 - acc: 0.6953 - val_loss: 0.6738 - val_acc: 0.7067\n",
      "Epoch 8/10\n",
      "4159000/4159000 [==============================] - 14s 3us/step - loss: 1.2224 - acc: 0.6966 - val_loss: 0.6737 - val_acc: 0.7087\n",
      "Epoch 9/10\n",
      "4159000/4159000 [==============================] - 13s 3us/step - loss: 1.2215 - acc: 0.6976 - val_loss: 0.6753 - val_acc: 0.7113\n",
      "Epoch 10/10\n",
      "4159000/4159000 [==============================] - 13s 3us/step - loss: 1.2209 - acc: 0.6975 - val_loss: 0.6686 - val_acc: 0.7150\n",
      "ALL patient scores: \n",
      "0.7579218106995885 [0.7957602818215063] [1.8596771219337553]\n",
      "Train on 4159000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "4159000/4159000 [==============================] - 12s 3us/step - loss: 1.2202 - acc: 0.6977 - val_loss: 0.6734 - val_acc: 0.7103\n",
      "Epoch 2/10\n",
      "4159000/4159000 [==============================] - 12s 3us/step - loss: 1.2198 - acc: 0.6976 - val_loss: 0.6753 - val_acc: 0.7090\n",
      "Epoch 3/10\n",
      "4159000/4159000 [==============================] - 13s 3us/step - loss: 1.2193 - acc: 0.6977 - val_loss: 0.6701 - val_acc: 0.7140\n",
      "Epoch 4/10\n",
      "4159000/4159000 [==============================] - 12s 3us/step - loss: 1.2190 - acc: 0.6984 - val_loss: 0.6788 - val_acc: 0.7040\n",
      "Epoch 5/10\n",
      "4159000/4159000 [==============================] - 13s 3us/step - loss: 1.2185 - acc: 0.6989 - val_loss: 0.6838 - val_acc: 0.6967\n",
      "Epoch 6/10\n",
      "4159000/4159000 [==============================] - 13s 3us/step - loss: 1.2183 - acc: 0.6981 - val_loss: 0.6825 - val_acc: 0.6970\n",
      "Epoch 7/10\n",
      "4159000/4159000 [==============================] - 13s 3us/step - loss: 1.2179 - acc: 0.6996 - val_loss: 0.6840 - val_acc: 0.6947\n",
      "Epoch 8/10\n",
      "4159000/4159000 [==============================] - 13s 3us/step - loss: 1.2177 - acc: 0.6989 - val_loss: 0.6753 - val_acc: 0.7103\n",
      "Epoch 9/10\n",
      "4159000/4159000 [==============================] - 12s 3us/step - loss: 1.2175 - acc: 0.6988 - val_loss: 0.6710 - val_acc: 0.7160\n",
      "Epoch 10/10\n",
      "4159000/4159000 [==============================] - 12s 3us/step - loss: 1.2173 - acc: 0.6993 - val_loss: 0.6799 - val_acc: 0.7040\n",
      "ALL patient scores: \n",
      "0.7628600823045268 [0.7957181303726731] [1.8956824527848186]\n",
      "Train on 4159000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "4159000/4159000 [==============================] - 12s 3us/step - loss: 1.2171 - acc: 0.6988 - val_loss: 0.6766 - val_acc: 0.7080\n",
      "Epoch 2/10\n",
      "4159000/4159000 [==============================] - 13s 3us/step - loss: 1.2168 - acc: 0.6997 - val_loss: 0.6692 - val_acc: 0.7143\n",
      "Epoch 3/10\n",
      "4159000/4159000 [==============================] - 12s 3us/step - loss: 1.2167 - acc: 0.6995 - val_loss: 0.6816 - val_acc: 0.7013\n",
      "Epoch 4/10\n",
      "4159000/4159000 [==============================] - 13s 3us/step - loss: 1.2167 - acc: 0.6988 - val_loss: 0.6621 - val_acc: 0.7290\n",
      "Epoch 5/10\n",
      "4159000/4159000 [==============================] - 13s 3us/step - loss: 1.2164 - acc: 0.6999 - val_loss: 0.6816 - val_acc: 0.7000\n",
      "Epoch 6/10\n",
      "4159000/4159000 [==============================] - 12s 3us/step - loss: 1.2163 - acc: 0.6996 - val_loss: 0.6866 - val_acc: 0.6980\n",
      "Epoch 7/10\n",
      "4159000/4159000 [==============================] - 12s 3us/step - loss: 1.2162 - acc: 0.6992 - val_loss: 0.6720 - val_acc: 0.7107\n",
      "Epoch 8/10\n",
      "4159000/4159000 [==============================] - 12s 3us/step - loss: 1.2161 - acc: 0.6995 - val_loss: 0.6742 - val_acc: 0.7117\n",
      "Epoch 9/10\n",
      "4159000/4159000 [==============================] - 11s 3us/step - loss: 1.2159 - acc: 0.6999 - val_loss: 0.6803 - val_acc: 0.7003\n",
      "Epoch 10/10\n",
      "4159000/4159000 [==============================] - 13s 3us/step - loss: 1.2158 - acc: 0.6992 - val_loss: 0.6801 - val_acc: 0.7050\n",
      "ALL patient scores: \n",
      "0.7617283950617284 [0.8315088853596267] [1.8629266273672946]\n",
      "Train on 4159000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "4159000/4159000 [==============================] - 14s 3us/step - loss: 1.2157 - acc: 0.7002 - val_loss: 0.6745 - val_acc: 0.7033\n",
      "Epoch 2/10\n",
      "4159000/4159000 [==============================] - 11s 3us/step - loss: 1.2157 - acc: 0.6992 - val_loss: 0.6861 - val_acc: 0.6943\n",
      "Epoch 3/10\n",
      "4159000/4159000 [==============================] - 12s 3us/step - loss: 1.2156 - acc: 0.6996 - val_loss: 0.6850 - val_acc: 0.6947\n",
      "Epoch 4/10\n",
      "4159000/4159000 [==============================] - 13s 3us/step - loss: 1.2154 - acc: 0.6995 - val_loss: 0.6818 - val_acc: 0.7103\n",
      "Epoch 5/10\n",
      "4159000/4159000 [==============================] - 13s 3us/step - loss: 1.2154 - acc: 0.7002 - val_loss: 0.6723 - val_acc: 0.7150\n",
      "Epoch 6/10\n",
      "4159000/4159000 [==============================] - 12s 3us/step - loss: 1.2153 - acc: 0.6993 - val_loss: 0.6714 - val_acc: 0.7107\n",
      "Epoch 7/10\n",
      "4159000/4159000 [==============================] - 13s 3us/step - loss: 1.2152 - acc: 0.6997 - val_loss: 0.6696 - val_acc: 0.7150\n",
      "Epoch 8/10\n",
      "4159000/4159000 [==============================] - 12s 3us/step - loss: 1.2152 - acc: 0.6999 - val_loss: 0.6640 - val_acc: 0.7240\n",
      "Epoch 9/10\n",
      "4159000/4159000 [==============================] - 12s 3us/step - loss: 1.2151 - acc: 0.6994 - val_loss: 0.6802 - val_acc: 0.7037\n",
      "Epoch 10/10\n",
      "4159000/4159000 [==============================] - 13s 3us/step - loss: 1.2151 - acc: 0.6998 - val_loss: 0.6768 - val_acc: 0.7093\n",
      "ALL patient scores: \n",
      "0.770679012345679 [0.868135180214959] [1.9327730257041513]\n",
      "Train on 4159000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "4159000/4159000 [==============================] - 13s 3us/step - loss: 1.2151 - acc: 0.6993 - val_loss: 0.6815 - val_acc: 0.7063\n",
      "Epoch 2/10\n",
      "4159000/4159000 [==============================] - 12s 3us/step - loss: 1.2151 - acc: 0.6993 - val_loss: 0.6877 - val_acc: 0.6947\n",
      "Epoch 3/10\n",
      "4159000/4159000 [==============================] - 13s 3us/step - loss: 1.2149 - acc: 0.6988 - val_loss: 0.6673 - val_acc: 0.7250\n",
      "Epoch 4/10\n",
      "4159000/4159000 [==============================] - 13s 3us/step - loss: 1.2148 - acc: 0.6997 - val_loss: 0.6600 - val_acc: 0.7327\n",
      "Epoch 5/10\n",
      "4159000/4159000 [==============================] - 13s 3us/step - loss: 1.2148 - acc: 0.7006 - val_loss: 0.6825 - val_acc: 0.6943\n",
      "Epoch 6/10\n",
      "4159000/4159000 [==============================] - 12s 3us/step - loss: 1.2147 - acc: 0.6982 - val_loss: 0.6687 - val_acc: 0.7280\n",
      "Epoch 7/10\n",
      "4159000/4159000 [==============================] - 13s 3us/step - loss: 1.2147 - acc: 0.7003 - val_loss: 0.6664 - val_acc: 0.7220\n",
      "Epoch 8/10\n",
      "4159000/4159000 [==============================] - 13s 3us/step - loss: 1.2139 - acc: 0.6977 - val_loss: 0.6692 - val_acc: 0.7150\n",
      "Epoch 9/10\n",
      "4159000/4159000 [==============================] - 12s 3us/step - loss: 1.2132 - acc: 0.6973 - val_loss: 0.6751 - val_acc: 0.7043\n",
      "Epoch 10/10\n",
      "4159000/4159000 [==============================] - 13s 3us/step - loss: 1.2129 - acc: 0.6974 - val_loss: 0.6726 - val_acc: 0.7060\n",
      "ALL patient scores: \n",
      "0.7670781893004115 [0.8111875087039793] [1.8883583292048969]\n"
     ]
    }
   ],
   "source": [
    "hrs = []\n",
    "discrete_hrs = []\n",
    "auc_vals = []\n",
    "for i in range(n_iter):\n",
    "    train_m()\n",
    "    \n",
    "    all_scores = score_m(all_test_patients)\n",
    "    cutoff = np.percentile(all_scores, 75)\n",
    "    all_discrete_scores = [1 if x >= cutoff else 0 for x in all_scores]\n",
    "\n",
    "    all_auc = evaluate_AUC(all_scores, all_test_patient_labels)\n",
    "    all_hr = evaluate_HR(survival_dict, scores, all_test_pids, all_test_patient_labels, \"continous\")\n",
    "    all_discrete_hr = evaluate_HR(survival_dict, all_discrete_scores, all_test_pids, all_test_patient_labels, \"discrete\")\n",
    "    \n",
    "    hrs.append(all_hr)\n",
    "    discrete_hrs.append(all_discrete_hr)\n",
    "    auc_vals.append(all_auc)\n",
    "    \n",
    "    print(\"ALL patient scores: \")\n",
    "    print(all_auc,all_hr, all_discrete_hr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = score_m(all_test_patients)\n",
    "cutoff = np.percentile(all_scores, 75)\n",
    "all_discrete_scores = [1 if x >= cutoff else 0 for x in all_scores]\n",
    "\n",
    "all_auc = evaluate_AUC(all_scores, all_test_patient_labels)\n",
    "all_hr = evaluate_HR(survival_dict, scores, all_test_pids, all_test_patient_labels, \"continous\")\n",
    "all_discrete_hr = evaluate_HR(survival_dict, all_discrete_scores, all_test_pids, all_test_patient_labels, \"discrete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "816"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_test_patients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use embedding_m to visualize\n",
    "    # which patients are predicted really correctly\n",
    "    # which patients are predicted very wrong\n",
    "    # what kinds of instances are very strong indicators\n",
    "    # what kinds of instances are a strong anti-indicators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4046,)\n",
      "(4046,)\n",
      "(4046,)\n",
      "(4046,)\n",
      "(4046,)\n"
     ]
    }
   ],
   "source": [
    "split_num = 0\n",
    "for split_num in range(5):\n",
    "    split_train_fname = \"./datasets/splits/split_\" + str(int(split_num)) + \"/train.h5\"\n",
    "    split_test_fname = \"./datasets/splits/split_\" + str(int(split_num)) + \"/test.h5\"\n",
    "\n",
    "    try:\n",
    "        #print(h5py.File(split_train_fname, \"r\").get(\"adjacent_beats\").shape)\n",
    "        print(h5py.File(split_train_fname, \"r\").get(\"pids\").shape)\n",
    "\n",
    "    except:\n",
    "        print(\"Didn't work: \", split_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Call--\n",
      "> /home/divyas/.local/lib/python3.6/site-packages/IPython/core/displayhook.py(247)__call__()\n",
      "-> def __call__(self, result=None):\n"
     ]
    }
   ],
   "source": [
    "y_mode = \"cvd\"\n",
    "splits = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "split_num = \"0\"\n",
    "split_dir = \"./datasets/splits/split_\" + split_num\n",
    "\n",
    "# Load Y\n",
    "y_train = np.loadtxt(split_dir + \"/\" + y_mode + \"_train_labels\")\n",
    "y_test = np.loadtxt(split_dir + \"/\" + y_mode + \"_test_labels\")\n",
    "\n",
    "\n",
    "x_train_file = h5py.File(split_dir + \"/train.h5\")\n",
    "x_test_file = h5py.File(split_dir + \"/test.h5\")\n",
    "X_train = np.array(x_train_file.get('adjacent_beats'))\n",
    "X_test = np.array(x_test_file.get('adjacent_beats'))\n",
    "x_train_file.close()\n",
    "x_test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
